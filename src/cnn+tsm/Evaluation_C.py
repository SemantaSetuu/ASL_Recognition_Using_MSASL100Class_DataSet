# eval_stage_c_topk.py  â€“ Stageâ€¯C CNNâ€‘Attn evaluator (topâ€‘1 / topâ€‘5)
from __future__ import annotations
import os, torch, torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm

# â”€â”€ silence C++ log spam â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["GLOG_minloglevel"]     = "3"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EDIT THESE PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT      = Path(r"C:\Users\seman\Desktop\clg\2nd_sem\research_practicum\code")
DATA_DIR  = ROOT / "data" / "images" / "test"          # or "val"
WEIGHTS   = ROOT / "src" / "cnn+tsm" / "stage_c_adv_cnn_attn.pth"
CLASS_TXT = ROOT / "src" / "cnn+tsm" / "stage_c_classes.txt"   # if you saved one
# ---------------------------------------------------------

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORT Stageâ€¯C definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€
#   Make sure this import points to the SAME file you used for training.
from stage_C_new import TemporalAttn, ASLSeqDataset, NUM_FRAMES, DEVICE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EVALUATION HELPER â”€â”€â”€â”€â”€â”€â”€â”€â”€
def eval_topk(model, loader, criterion: nn.Module | None = None):
    model.eval(); top1=top5=tot=loss=0.
    bar = tqdm(total=len(loader.dataset), unit="vid",
               desc=f"Testing {split_name}", ncols=80)
    with torch.no_grad():
        for clip, lab in loader:
            clip, lab = clip.to(DEVICE), lab.to(DEVICE)
            out = model(clip)
            if criterion:
                loss += criterion(out, lab).item() * lab.size(0)
            tot += lab.size(0)

            _, pred = out.topk(5, 1, True, True)
            corr    = pred.t().eq(lab.view(1, -1).expand_as(pred.t()))
            top1   += corr[:1].flatten().sum().item()
            top5   += corr[:5].flatten().sum().item()

            bar.update(lab.size(0))
    bar.close()
    return ((loss/tot) if criterion else None, 100*top1/tot, 100*top5/tot)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN (Windowsâ€‘safe) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    global split_name
    split_name = DATA_DIR.parts[-1]           # "val" or "test"

    # â€” class list / output size â€”
    if CLASS_TXT.exists():
        cls_list = [l.strip() for l in CLASS_TXT.open(encoding="utfâ€‘8") if l.strip()]
        n_cls = len(cls_list)
    else:
        n_cls = torch.load(WEIGHTS, map_location="cpu")["fc.weight"].shape[0]

    # â€” dataset & loader â€”
    ds = ASLSeqDataset(DATA_DIR, split="val", T=NUM_FRAMES)
    if len(ds) == 0:
        raise RuntimeError(f"No clips found in {DATA_DIR}. "
                           "Did you extract frames for this split?")
    print(f"ğŸ—‚ï¸  {split_name.capitalize()} set : {len(ds)} videos   "
          f"{len(ds.cls)}/{n_cls} classes")

    loader = DataLoader(ds, batch_size=4, shuffle=False,
                        num_workers=4, pin_memory=True)

    # â€” model â€”
    model = TemporalAttn(n_cls).to(DEVICE)
    model.load_state_dict(torch.load(WEIGHTS, map_location=DEVICE), strict=True)

    # â€” evaluate â€”
    loss, top1, top5 = eval_topk(model, loader, nn.CrossEntropyLoss())
    print(f"\nğŸ…  Topâ€‘1 accuracy : {top1:6.2f}%")
    print(f"ğŸ¥ˆ  Topâ€‘5 accuracy : {top5:6.2f}%")
    print(f"ğŸ“‰  {split_name.capitalize()} loss : {loss:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENTRY POINT â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    torch.multiprocessing.set_start_method("spawn", force=True)  # Windows
    main()
